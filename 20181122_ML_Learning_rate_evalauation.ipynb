{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning_rate 에 따른 학습\n",
    "- Case1 : learning_rate = 0.1\n",
    "- Case2 : learning_rate = 1.5\n",
    "- Case3 : learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case1 : learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]\n",
    "y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy   = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb6ca0315484f6a9a35c1b5933f01b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step : 0 \n",
      " Cost : 4.536120414733887 \n",
      " Weight : \n",
      "[[-1.0684217   0.78146774  1.2702483 ]\n",
      " [ 1.3805251   1.1117564  -0.945031  ]\n",
      " [-0.64199984 -1.0010467  -0.9006645 ]]\n",
      "\n",
      " Step : 20 \n",
      " Cost : 1.0336236953735352 \n",
      " Weight : \n",
      "[[-1.2362244   0.5305092   1.6890093 ]\n",
      " [ 0.92547345  0.5995444   0.02223267]\n",
      " [-0.90003335 -1.0532573  -0.5904203 ]]\n",
      "\n",
      " Step : 40 \n",
      " Cost : 0.8368585109710693 \n",
      " Weight : \n",
      "[[-1.3268209   0.40004876  1.910066  ]\n",
      " [ 0.8295872   0.46504766  0.25261572]\n",
      " [-0.7752031  -0.86927366 -0.8992343 ]]\n",
      "\n",
      " Step : 60 \n",
      " Cost : 0.746263325214386 \n",
      " Weight : \n",
      "[[-1.4129989   0.30010444  2.0961885 ]\n",
      " [ 0.7634871   0.4230427   0.36072057]\n",
      " [-0.67249227 -0.7852348  -1.0859839 ]]\n",
      "\n",
      " Step : 80 \n",
      " Cost : 0.6942170858383179 \n",
      " Weight : \n",
      "[[-1.4956346   0.22442693  2.2545016 ]\n",
      " [ 0.7198366   0.41781348  0.40960038]\n",
      " [-0.5929298  -0.7455856  -1.2051957 ]]\n",
      "\n",
      " Step : 100 \n",
      " Cost : 0.6588690280914307 \n",
      " Weight : \n",
      "[[-1.5756699   0.16674821  2.3922162 ]\n",
      " [ 0.69244117  0.42465964  0.43014985]\n",
      " [-0.5313173  -0.72495973 -1.287434  ]]\n",
      "\n",
      " Step : 120 \n",
      " Cost : 0.6322969198226929 \n",
      " Weight : \n",
      "[[-1.6535165   0.12245124  2.5143602 ]\n",
      " [ 0.6761472   0.43453723  0.4365661 ]\n",
      " [-0.48236957 -0.7128993  -1.3484421 ]]\n",
      "\n",
      " Step : 140 \n",
      " Cost : 0.6109997034072876 \n",
      " Weight : \n",
      "[[-1.7293099   0.08824365  2.624361  ]\n",
      " [ 0.6670407   0.44438082  0.43582895]\n",
      " [-0.44195378 -0.7050846  -1.3966726 ]]\n",
      "\n",
      " Step : 160 \n",
      " Cost : 0.5931869745254517 \n",
      " Weight : \n",
      "[[-1.8031102   0.06176718  2.7246375 ]\n",
      " [ 0.6623823   0.4533008   0.43156734]\n",
      " [-0.4071634  -0.69966453 -1.4368831 ]]\n",
      "\n",
      " Step : 180 \n",
      " Cost : 0.5778399705886841 \n",
      " Weight : \n",
      "[[-1.8749808   0.04131234  2.8169634 ]\n",
      " [ 0.6603663   0.46115905  0.42572495]\n",
      " [-0.3760687  -0.69578916 -1.4718533 ]]\n",
      "\n",
      " Step : 200 \n",
      " Cost : 0.5643270015716553 \n",
      " Weight : \n",
      "[[-1.9450043   0.0256207   2.9026787 ]\n",
      " [ 0.65985155  0.4680491   0.4193497 ]\n",
      " [-0.34742978 -0.69302523 -1.5032561 ]]\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prediction \t: [2 2 2]\n",
      "Accuracy \t:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in tqdm_notebook(range(201)):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        if step%20 == 0:\n",
    "            print(\"\\n Step : {} \\n Cost : {} \\n Weight : \\n{}\".format(step, cost_val, W_val))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    # predict\n",
    "    print(\"Prediction \\t:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy \\t: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case2 : learning_rate = 1.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "learning_rate = 1.5\n",
    "\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]\n",
    "y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step : 0 \n",
      " Cost : 3.1290576457977295 \n",
      " Weight : \n",
      "[[-1.0138175   0.13082397  2.1564288 ]\n",
      " [ 2.519365   -3.3371594   1.0621253 ]\n",
      " [ 3.2201056  -1.9236073   1.1348552 ]]\n",
      "\n",
      " Step : 1 \n",
      " Cost : 22.944168090820312 \n",
      " Weight : \n",
      "[[-2.1261158   0.69332397  2.7062273 ]\n",
      " [-1.5798447  -0.7121594   2.5363357 ]\n",
      " [-0.89178133  0.88889265  2.4342427 ]]\n",
      "\n",
      " Step : 2 \n",
      " Cost : 23.602142333984375 \n",
      " Weight : \n",
      "[[-1.7511158  1.2558229  1.7687284]\n",
      " [ 0.8576553  1.9128385 -2.5261624]\n",
      " [ 1.5457187  3.7013917 -2.8157563]]\n",
      "\n",
      " Step : 3 \n",
      " Cost : 15.480286598205566 \n",
      " Weight : \n",
      "[[-1.3764788   0.318686    2.3312283 ]\n",
      " [ 3.294415   -2.023921   -1.0261626 ]\n",
      " [ 3.9828405  -0.04823017 -1.5032564 ]]\n",
      "\n",
      " Step : 4 \n",
      " Cost : 24.930477142333984 \n",
      " Weight : \n",
      "[[-2.5014744   0.88118595  2.893724  ]\n",
      " [-0.8305764   0.601079    0.47382903]\n",
      " [-0.14215517  2.7642698  -0.19076061]]\n",
      "\n",
      " Step : 20 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 40 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 60 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 80 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 100 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 120 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 140 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 160 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 180 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      " Step : 200 \n",
      " Cost : nan \n",
      " Weight : \n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "------------------------------------------------------------\n",
      "Prediction \t: [0 0 0]\n",
      "Accuracy \t:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        if step%20 == 0 or step<5:\n",
    "            print(\"\\n Step : {} \\n Cost : {} \\n Weight : \\n{}\".format(step, cost_val, W_val))\n",
    "    print(\"-\"*60)\n",
    "    # predict\n",
    "    print(\"Prediction \\t:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy \\t: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case3 : learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "learning_rate = 1e-5\n",
    "\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]\n",
    "y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step : 0 \n",
      " Cost : 3.456878185272217 \n",
      " Weight : \n",
      "[[ 1.0042118  -2.1880436  -0.911383  ]\n",
      " [ 1.6207315  -0.06806801  2.578161  ]\n",
      " [-1.5332353   1.1768539  -1.0610433 ]]\n",
      "\n",
      " Step : 20 \n",
      " Cost : 3.45285701751709 \n",
      " Weight : \n",
      "[[ 1.0042523 -2.1880102 -0.9114533]\n",
      " [ 1.6210319 -0.0678333  2.577627 ]\n",
      " [-1.5329254  1.1770256 -1.0615249]]\n",
      "\n",
      " Step : 40 \n",
      " Cost : 3.4488391876220703 \n",
      " Weight : \n",
      "[[ 1.0042928  -2.1879768  -0.91152364]\n",
      " [ 1.6213323  -0.06759885  2.577093  ]\n",
      " [-1.5326154   1.1771972  -1.0620065 ]]\n",
      "\n",
      " Step : 60 \n",
      " Cost : 3.444823741912842 \n",
      " Weight : \n",
      "[[ 1.0043334  -2.1879435  -0.911594  ]\n",
      " [ 1.6216327  -0.06736467  2.5765588 ]\n",
      " [-1.5323055   1.1773689  -1.0624881 ]]\n",
      "\n",
      " Step : 80 \n",
      " Cost : 3.440812110900879 \n",
      " Weight : \n",
      "[[ 1.0043739  -2.18791    -0.9116643 ]\n",
      " [ 1.6219331  -0.06713074  2.5760248 ]\n",
      " [-1.5319955   1.1775393  -1.0629697 ]]\n",
      "\n",
      " Step : 100 \n",
      " Cost : 3.4368085861206055 \n",
      " Weight : \n",
      "[[ 1.0044144  -2.1878767  -0.91173464]\n",
      " [ 1.6222335  -0.06689708  2.5754907 ]\n",
      " [-1.5316856   1.1777086  -1.0634495 ]]\n",
      "\n",
      " Step : 120 \n",
      " Cost : 3.432809829711914 \n",
      " Weight : \n",
      "[[ 1.004455   -2.1878452  -0.911805  ]\n",
      " [ 1.6225339  -0.06666367  2.5749567 ]\n",
      " [-1.5313756   1.1778779  -1.0639287 ]]\n",
      "\n",
      " Step : 140 \n",
      " Cost : 3.428814172744751 \n",
      " Weight : \n",
      "[[ 1.0044955  -2.1878166  -0.9118753 ]\n",
      " [ 1.6228343  -0.06643053  2.5744226 ]\n",
      " [-1.5310657   1.1780472  -1.064408  ]]\n",
      "\n",
      " Step : 160 \n",
      " Cost : 3.424821376800537 \n",
      " Weight : \n",
      "[[ 1.0045352  -2.187788   -0.91194564]\n",
      " [ 1.6231347  -0.06619766  2.5738885 ]\n",
      " [-1.5307558   1.1782165  -1.0648872 ]]\n",
      "\n",
      " Step : 180 \n",
      " Cost : 3.4208321571350098 \n",
      " Weight : \n",
      "[[ 1.0045733  -2.1877594  -0.912016  ]\n",
      " [ 1.6234351  -0.06596504  2.5733545 ]\n",
      " [-1.5304458   1.1783857  -1.0653664 ]]\n",
      "\n",
      " Step : 200 \n",
      " Cost : 3.4168453216552734 \n",
      " Weight : \n",
      "[[ 1.0046115  -2.1877308  -0.9120863 ]\n",
      " [ 1.6237355  -0.06573268  2.5728204 ]\n",
      " [-1.5301359   1.178555   -1.0658456 ]]\n",
      "------------------------------------------------------------\n",
      "Prediction \t: [0 0 0]\n",
      "Accuracy \t:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        if step%20 == 0:\n",
    "            print(\"\\n Step : {} \\n Cost : {} \\n Weight : \\n{}\".format(step, cost_val, W_val))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    # predict\n",
    "    print(\"Prediction \\t:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy \\t: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data(x) preprocessing for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_data : \n",
      "[[8.28659973e+02 8.33450012e+02 9.08100000e+05 8.28349976e+02]\n",
      " [8.23020020e+02 8.28070007e+02 1.82810000e+06 8.21655029e+02]\n",
      " [8.19929993e+02 8.24400024e+02 1.43810000e+06 8.18979980e+02]\n",
      " [8.16000000e+02 8.20958984e+02 1.00810000e+06 8.15489990e+02]\n",
      " [8.19359985e+02 8.23000000e+02 1.18810000e+06 8.18469971e+02]\n",
      " [8.19000000e+02 8.23000000e+02 1.19810000e+06 8.16000000e+02]\n",
      " [8.11700012e+02 8.15250000e+02 1.09810000e+06 8.09780029e+02]\n",
      " [8.09510010e+02 8.16659973e+02 1.39810000e+06 8.04539978e+02]]  \n",
      "\n",
      " y_data : \n",
      "[[831.659973]\n",
      " [828.070007]\n",
      " [824.159973]\n",
      " [819.23999 ]\n",
      " [818.97998 ]\n",
      " [820.450012]\n",
      " [813.669983]\n",
      " [809.559998]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100,  828.349976, 831.659973],\n",
    "               [823.02002,  828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998,  824.159973],\n",
    "               [816,        820.958984, 1008100, 815.48999,  819.23999 ],\n",
    "               [819.359985, 823,        1188100, 818.469971, 818.97998 ],\n",
    "               [819,        823,        1198100, 816,        820.450012],\n",
    "               [811.700012, 815.25,     1098100, 809.780029, 813.669983],\n",
    "               [809.51001,  816.659973, 1398100, 804.539978, 809.559998]])\n",
    "# print(xy)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "print(\" x_data : \\n{}  \\n\\n y_data : \\n{}\".format(x_data, y_data))\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : 0, \t Cost : 2859054661632.0\n",
      "Step : 1, \t Cost : 3.1411874338828935e+27\n",
      "Step : 2, \t Cost : inf\n",
      "Step : 3, \t Cost : inf\n",
      "Step : 4, \t Cost : inf\n",
      "Step : 5, \t Cost : inf\n",
      "Step : 6, \t Cost : nan\n",
      "Step : 7, \t Cost : nan\n",
      "Step : 8, \t Cost : nan\n",
      "Step : 9, \t Cost : nan\n",
      "Step : 10, \t Cost : nan\n",
      "Step : 11, \t Cost : nan\n",
      "Step : 12, \t Cost : nan\n",
      "Step : 13, \t Cost : nan\n",
      "Step : 14, \t Cost : nan\n",
      "Step : 15, \t Cost : nan\n",
      "Step : 16, \t Cost : nan\n",
      "Step : 17, \t Cost : nan\n",
      "Step : 18, \t Cost : nan\n",
      "Step : 19, \t Cost : nan\n",
      "Step : 20, \t Cost : nan\n",
      "Step : 21, \t Cost : nan\n",
      "Step : 22, \t Cost : nan\n",
      "Step : 23, \t Cost : nan\n",
      "Step : 24, \t Cost : nan\n",
      "Step : 25, \t Cost : nan\n",
      "Step : 26, \t Cost : nan\n",
      "Step : 27, \t Cost : nan\n",
      "Step : 28, \t Cost : nan\n",
      "Step : 29, \t Cost : nan\n",
      "Step : 30, \t Cost : nan\n",
      "Step : 31, \t Cost : nan\n",
      "Step : 32, \t Cost : nan\n",
      "Step : 33, \t Cost : nan\n",
      "Step : 34, \t Cost : nan\n",
      "Step : 35, \t Cost : nan\n",
      "Step : 36, \t Cost : nan\n",
      "Step : 37, \t Cost : nan\n",
      "Step : 38, \t Cost : nan\n",
      "Step : 39, \t Cost : nan\n",
      "Step : 40, \t Cost : nan\n",
      "Step : 41, \t Cost : nan\n",
      "Step : 42, \t Cost : nan\n",
      "Step : 43, \t Cost : nan\n",
      "Step : 44, \t Cost : nan\n",
      "Step : 45, \t Cost : nan\n",
      "Step : 46, \t Cost : nan\n",
      "Step : 47, \t Cost : nan\n",
      "Step : 48, \t Cost : nan\n",
      "Step : 49, \t Cost : nan\n",
      "Step : 50, \t Cost : nan\n",
      "Step : 51, \t Cost : nan\n",
      "Step : 52, \t Cost : nan\n",
      "Step : 53, \t Cost : nan\n",
      "Step : 54, \t Cost : nan\n",
      "Step : 55, \t Cost : nan\n",
      "Step : 56, \t Cost : nan\n",
      "Step : 57, \t Cost : nan\n",
      "Step : 58, \t Cost : nan\n",
      "Step : 59, \t Cost : nan\n",
      "Step : 60, \t Cost : nan\n",
      "Step : 61, \t Cost : nan\n",
      "Step : 62, \t Cost : nan\n",
      "Step : 63, \t Cost : nan\n",
      "Step : 64, \t Cost : nan\n",
      "Step : 65, \t Cost : nan\n",
      "Step : 66, \t Cost : nan\n",
      "Step : 67, \t Cost : nan\n",
      "Step : 68, \t Cost : nan\n",
      "Step : 69, \t Cost : nan\n",
      "Step : 70, \t Cost : nan\n",
      "Step : 71, \t Cost : nan\n",
      "Step : 72, \t Cost : nan\n",
      "Step : 73, \t Cost : nan\n",
      "Step : 74, \t Cost : nan\n",
      "Step : 75, \t Cost : nan\n",
      "Step : 76, \t Cost : nan\n",
      "Step : 77, \t Cost : nan\n",
      "Step : 78, \t Cost : nan\n",
      "Step : 79, \t Cost : nan\n",
      "Step : 80, \t Cost : nan\n",
      "Step : 81, \t Cost : nan\n",
      "Step : 82, \t Cost : nan\n",
      "Step : 83, \t Cost : nan\n",
      "Step : 84, \t Cost : nan\n",
      "Step : 85, \t Cost : nan\n",
      "Step : 86, \t Cost : nan\n",
      "Step : 87, \t Cost : nan\n",
      "Step : 88, \t Cost : nan\n",
      "Step : 89, \t Cost : nan\n",
      "Step : 90, \t Cost : nan\n",
      "Step : 91, \t Cost : nan\n",
      "Step : 92, \t Cost : nan\n",
      "Step : 93, \t Cost : nan\n",
      "Step : 94, \t Cost : nan\n",
      "Step : 95, \t Cost : nan\n",
      "Step : 96, \t Cost : nan\n",
      "Step : 97, \t Cost : nan\n",
      "Step : 98, \t Cost : nan\n",
      "Step : 99, \t Cost : nan\n",
      "Step : 100, \t Cost : nan\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"Step : {}, \\t Cost : {}\".format(step, cost_val))\n",
    "    # print(\"\\n step : {}, \\n Cost : {} \\n Prediction : {}\".format(step, cost_val, hy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with min, max\n",
    "- 위 처럼 데이터간 값 차이가 너무 큰 경우 nan이 떠버림\n",
    "- 그걸 해결하기 위해 min max 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(4).reshape((2,2))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max.data :\n",
      " [8.28659973e+02 8.33450012e+02 1.82810000e+06 8.28349976e+02\n",
      " 8.31659973e+02], \n",
      "\n",
      " min.data :\n",
      " [8.09510010e+02 8.15250000e+02 9.08100000e+05 8.04539978e+02\n",
      " 8.09559998e+02], \n",
      "\n",
      " numerator :\n",
      " [[1.9149963e+01 1.8200012e+01 0.0000000e+00 2.3809998e+01 2.2099975e+01]\n",
      " [1.3510010e+01 1.2820007e+01 9.2000000e+05 1.7115051e+01 1.8510009e+01]\n",
      " [1.0419983e+01 9.1500240e+00 5.3000000e+05 1.4440002e+01 1.4599975e+01]\n",
      " [6.4899900e+00 5.7089840e+00 1.0000000e+05 1.0950012e+01 9.6799920e+00]\n",
      " [9.8499750e+00 7.7500000e+00 2.8000000e+05 1.3929993e+01 9.4199820e+00]\n",
      " [9.4899900e+00 7.7500000e+00 2.9000000e+05 1.1460022e+01 1.0890014e+01]\n",
      " [2.1900020e+00 0.0000000e+00 1.9000000e+05 5.2400510e+00 4.1099850e+00]\n",
      " [0.0000000e+00 1.4099730e+00 4.9000000e+05 0.0000000e+00 0.0000000e+00]], \n",
      "\n",
      " denominator :\n",
      " [1.9149963e+01 1.8200012e+01 9.2000000e+05 2.3809998e+01 2.2099975e+01] \n",
      "\n",
      " xy : MinMaxScaler(xy) = numerator / (denominator + 1e-7) \n",
      " [[0.99999948 0.99999945 0.         0.99999958 0.99999955]\n",
      " [0.70548455 0.70439514 1.         0.71881752 0.83755754]\n",
      " [0.54412521 0.50274796 0.57608696 0.60646775 0.6606328 ]\n",
      " [0.33890335 0.31368006 0.10869565 0.45989115 0.43800899]\n",
      " [0.51435973 0.42582366 0.30434783 0.58504781 0.42624382]\n",
      " [0.49556153 0.42582366 0.31521739 0.48131114 0.49276115]\n",
      " [0.11436058 0.         0.20652174 0.22007767 0.1859723 ]\n",
      " [0.         0.07747095 0.5326087  0.         0.        ]]\n",
      "\n",
      " x_data : \n",
      "[[0.99999948 0.99999945 0.         0.99999958]\n",
      " [0.70548455 0.70439514 1.         0.71881752]\n",
      " [0.54412521 0.50274796 0.57608696 0.60646775]\n",
      " [0.33890335 0.31368006 0.10869565 0.45989115]\n",
      " [0.51435973 0.42582366 0.30434783 0.58504781]\n",
      " [0.49556153 0.42582366 0.31521739 0.48131114]\n",
      " [0.11436058 0.         0.20652174 0.22007767]\n",
      " [0.         0.07747095 0.5326087  0.        ]]  \n",
      "\n",
      " y_data : \n",
      "[[0.99999955]\n",
      " [0.83755754]\n",
      " [0.6606328 ]\n",
      " [0.43800899]\n",
      " [0.42624382]\n",
      " [0.49276115]\n",
      " [0.1859723 ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    print(\" max.data :\\n {max}, \\n\\n min.data :\\n {min}, \\n\\n numerator :\\n {numerator}, \\n\\n denominator :\\n {denominator} \\n\".format(\n",
    "            max = np.max(data, 0), \n",
    "            min = np.min(data, 0),\n",
    "            numerator = numerator,\n",
    "            denominator = denominator\n",
    "        ))\n",
    "\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-5)\n",
    "\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100,  828.349976, 831.659973],\n",
    "               [823.02002,  828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998,  824.159973],\n",
    "               [816,        820.958984, 1008100, 815.48999,  819.23999 ],\n",
    "               [819.359985, 823,        1188100, 818.469971, 818.97998 ],\n",
    "               [819,        823,        1198100, 816,        820.450012],\n",
    "               [811.700012, 815.25,     1098100, 809.780029, 813.669983],\n",
    "               [809.51001,  816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "# print(\"np.max(xy, 0) :\", np.max(xy, 0))\n",
    "# print(\"np.min(xy, 0) :\", np.min(xy, 0))\n",
    "\n",
    "# very important. It does not work without it.\n",
    "xy = MinMaxScaler(xy)\n",
    "print(\" xy : MinMaxScaler(xy) = numerator / (denominator + 1e-7) \\n\", xy)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "print(\"\\n x_data : \\n{}  \\n\\n y_data : \\n{}\".format(x_data, y_data))\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1184f1a09954d4fa80146f1013790f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : 0, \t Cost : 1.1190135478973389, \t Improvement : -0.11901354789733887\n",
      "Step : 10, \t Cost : 1.1183431148529053, \t Improvement : 0.0006704330444335938\n",
      "Step : 20, \t Cost : 1.1176725625991821, \t Improvement : 0.0006705522537231445\n",
      "Step : 30, \t Cost : 1.1170027256011963, \t Improvement : 0.0006698369979858398\n",
      "Step : 40, \t Cost : 1.1163330078125, \t Improvement : 0.0006697177886962891\n",
      "Step : 50, \t Cost : 1.115663766860962, \t Improvement : 0.0006692409515380859\n",
      "Step : 60, \t Cost : 1.1149952411651611, \t Improvement : 0.0006685256958007812\n",
      "Step : 70, \t Cost : 1.1143275499343872, \t Improvement : 0.0006676912307739258\n",
      "Step : 80, \t Cost : 1.1136600971221924, \t Improvement : 0.0006674528121948242\n",
      "Step : 90, \t Cost : 1.112992763519287, \t Improvement : 0.0006673336029052734\n",
      "Step : 100, \t Cost : 1.11232590675354, \t Improvement : 0.0006668567657470703\n",
      "Step : 110, \t Cost : 1.1116594076156616, \t Improvement : 0.000666499137878418\n",
      "Step : 120, \t Cost : 1.1109933853149414, \t Improvement : 0.0006660223007202148\n",
      "Step : 130, \t Cost : 1.1103274822235107, \t Improvement : 0.0006659030914306641\n",
      "Step : 140, \t Cost : 1.1096622943878174, \t Improvement : 0.0006651878356933594\n",
      "Step : 150, \t Cost : 1.1089977025985718, \t Improvement : 0.0006645917892456055\n",
      "Step : 160, \t Cost : 1.1083357334136963, \t Improvement : 0.0006619691848754883\n",
      "Step : 170, \t Cost : 1.1076741218566895, \t Improvement : 0.0006616115570068359\n",
      "Step : 180, \t Cost : 1.1070131063461304, \t Improvement : 0.000661015510559082\n",
      "Step : 190, \t Cost : 1.1063519716262817, \t Improvement : 0.0006611347198486328\n",
      "Step : 200, \t Cost : 1.1056915521621704, \t Improvement : 0.0006604194641113281\n",
      "Step : 210, \t Cost : 1.1050313711166382, \t Improvement : 0.0006601810455322266\n",
      "Step : 220, \t Cost : 1.104371428489685, \t Improvement : 0.000659942626953125\n",
      "Step : 230, \t Cost : 1.1037118434906006, \t Improvement : 0.0006595849990844727\n",
      "Step : 240, \t Cost : 1.1030523777008057, \t Improvement : 0.0006594657897949219\n",
      "Step : 250, \t Cost : 1.1023931503295898, \t Improvement : 0.0006592273712158203\n",
      "Step : 260, \t Cost : 1.1017343997955322, \t Improvement : 0.0006587505340576172\n",
      "Step : 270, \t Cost : 1.1010758876800537, \t Improvement : 0.0006585121154785156\n",
      "Step : 280, \t Cost : 1.1004176139831543, \t Improvement : 0.0006582736968994141\n",
      "Step : 290, \t Cost : 1.0997600555419922, \t Improvement : 0.0006575584411621094\n",
      "Step : 300, \t Cost : 1.099102258682251, \t Improvement : 0.0006577968597412109\n",
      "Step : 310, \t Cost : 1.098444938659668, \t Improvement : 0.0006573200225830078\n",
      "Step : 320, \t Cost : 1.0977879762649536, \t Improvement : 0.0006569623947143555\n",
      "Step : 330, \t Cost : 1.0971325635910034, \t Improvement : 0.0006554126739501953\n",
      "Step : 340, \t Cost : 1.0964784622192383, \t Improvement : 0.0006541013717651367\n",
      "Step : 350, \t Cost : 1.0958245992660522, \t Improvement : 0.0006538629531860352\n",
      "Step : 360, \t Cost : 1.0951710939407349, \t Improvement : 0.0006535053253173828\n",
      "Step : 370, \t Cost : 1.094518780708313, \t Improvement : 0.000652313232421875\n",
      "Step : 380, \t Cost : 1.0938669443130493, \t Improvement : 0.0006518363952636719\n",
      "Step : 390, \t Cost : 1.0932154655456543, \t Improvement : 0.0006514787673950195\n",
      "Step : 400, \t Cost : 1.0925642251968384, \t Improvement : 0.000651240348815918\n",
      "Step : 410, \t Cost : 1.0919133424758911, \t Improvement : 0.0006508827209472656\n",
      "Step : 420, \t Cost : 1.0912625789642334, \t Improvement : 0.0006507635116577148\n",
      "Step : 430, \t Cost : 1.0906121730804443, \t Improvement : 0.0006504058837890625\n",
      "Step : 440, \t Cost : 1.0899620056152344, \t Improvement : 0.0006501674652099609\n",
      "Step : 450, \t Cost : 1.0893123149871826, \t Improvement : 0.0006496906280517578\n",
      "Step : 460, \t Cost : 1.088663101196289, \t Improvement : 0.0006492137908935547\n",
      "Step : 470, \t Cost : 1.0880138874053955, \t Improvement : 0.0006492137908935547\n",
      "Step : 480, \t Cost : 1.0873650312423706, \t Improvement : 0.0006488561630249023\n",
      "Step : 490, \t Cost : 1.0867164134979248, \t Improvement : 0.0006486177444458008\n",
      "Step : 500, \t Cost : 1.0860689878463745, \t Improvement : 0.000647425651550293\n",
      "Step : 510, \t Cost : 1.0854241847991943, \t Improvement : 0.0006448030471801758\n",
      "Step : 520, \t Cost : 1.0847795009613037, \t Improvement : 0.000644683837890625\n",
      "Step : 530, \t Cost : 1.0841352939605713, \t Improvement : 0.0006442070007324219\n",
      "Step : 540, \t Cost : 1.0834914445877075, \t Improvement : 0.0006438493728637695\n",
      "Step : 550, \t Cost : 1.0828478336334229, \t Improvement : 0.000643610954284668\n",
      "Step : 560, \t Cost : 1.0822045803070068, \t Improvement : 0.0006432533264160156\n",
      "Step : 570, \t Cost : 1.08156156539917, \t Improvement : 0.0006430149078369141\n",
      "Step : 580, \t Cost : 1.080918788909912, \t Improvement : 0.0006427764892578125\n",
      "Step : 590, \t Cost : 1.0802761316299438, \t Improvement : 0.0006426572799682617\n",
      "Step : 600, \t Cost : 1.0796338319778442, \t Improvement : 0.0006422996520996094\n",
      "Step : 610, \t Cost : 1.0789918899536133, \t Improvement : 0.000641942024230957\n",
      "Step : 620, \t Cost : 1.0783504247665405, \t Improvement : 0.0006414651870727539\n",
      "Step : 630, \t Cost : 1.0777090787887573, \t Improvement : 0.0006413459777832031\n",
      "Step : 640, \t Cost : 1.0770680904388428, \t Improvement : 0.0006409883499145508\n",
      "Step : 650, \t Cost : 1.0764273405075073, \t Improvement : 0.0006407499313354492\n",
      "Step : 660, \t Cost : 1.075786828994751, \t Improvement : 0.0006405115127563477\n",
      "Step : 670, \t Cost : 1.0751466751098633, \t Improvement : 0.0006401538848876953\n",
      "Step : 680, \t Cost : 1.0745084285736084, \t Improvement : 0.0006382465362548828\n",
      "Step : 690, \t Cost : 1.0738720893859863, \t Improvement : 0.0006363391876220703\n",
      "Step : 700, \t Cost : 1.0732362270355225, \t Improvement : 0.0006358623504638672\n",
      "Step : 710, \t Cost : 1.0726006031036377, \t Improvement : 0.0006356239318847656\n",
      "Step : 720, \t Cost : 1.0719653367996216, \t Improvement : 0.0006352663040161133\n",
      "Step : 730, \t Cost : 1.0713303089141846, \t Improvement : 0.0006350278854370117\n",
      "Step : 740, \t Cost : 1.070695400238037, \t Improvement : 0.0006349086761474609\n",
      "Step : 750, \t Cost : 1.0700609683990479, \t Improvement : 0.0006344318389892578\n",
      "Step : 760, \t Cost : 1.0694265365600586, \t Improvement : 0.0006344318389892578\n",
      "Step : 770, \t Cost : 1.0687925815582275, \t Improvement : 0.0006339550018310547\n",
      "Step : 780, \t Cost : 1.0681589841842651, \t Improvement : 0.0006335973739624023\n",
      "Step : 790, \t Cost : 1.0675256252288818, \t Improvement : 0.0006333589553833008\n",
      "Step : 800, \t Cost : 1.0668925046920776, \t Improvement : 0.0006331205368041992\n",
      "Step : 810, \t Cost : 1.066259741783142, \t Improvement : 0.0006327629089355469\n",
      "Step : 820, \t Cost : 1.0656273365020752, \t Improvement : 0.0006324052810668945\n",
      "Step : 830, \t Cost : 1.0649950504302979, \t Improvement : 0.0006322860717773438\n",
      "Step : 840, \t Cost : 1.0643630027770996, \t Improvement : 0.0006320476531982422\n",
      "Step : 850, \t Cost : 1.0637314319610596, \t Improvement : 0.0006315708160400391\n",
      "Step : 860, \t Cost : 1.0631024837493896, \t Improvement : 0.0006289482116699219\n",
      "Step : 870, \t Cost : 1.0624746084213257, \t Improvement : 0.0006278753280639648\n",
      "Step : 880, \t Cost : 1.0618470907211304, \t Improvement : 0.0006275177001953125\n",
      "Step : 890, \t Cost : 1.0612196922302246, \t Improvement : 0.0006273984909057617\n",
      "Step : 900, \t Cost : 1.0605926513671875, \t Improvement : 0.0006270408630371094\n",
      "Step : 910, \t Cost : 1.05996572971344, \t Improvement : 0.0006269216537475586\n",
      "Step : 920, \t Cost : 1.0593392848968506, \t Improvement : 0.0006264448165893555\n",
      "Step : 930, \t Cost : 1.0587129592895508, \t Improvement : 0.0006263256072998047\n",
      "Step : 940, \t Cost : 1.0580869913101196, \t Improvement : 0.0006259679794311523\n",
      "Step : 950, \t Cost : 1.0574612617492676, \t Improvement : 0.0006257295608520508\n",
      "Step : 960, \t Cost : 1.0568358898162842, \t Improvement : 0.0006253719329833984\n",
      "Step : 970, \t Cost : 1.0562108755111694, \t Improvement : 0.0006250143051147461\n",
      "Step : 980, \t Cost : 1.0555860996246338, \t Improvement : 0.0006247758865356445\n",
      "Step : 990, \t Cost : 1.0549615621566772, \t Improvement : 0.000624537467956543\n",
      "Step : 1000, \t Cost : 1.0543382167816162, \t Improvement : 0.0006233453750610352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "prev_cost = 1\n",
    "for step in tqdm_notebook(range(1001)):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    \n",
    "    if step%10 == 0:\n",
    "        # print(\"\\nStep\", step, \"\\nCost:\", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "        # print(\"Step : {}, \\t Cost : {}\".format(step, cost_val))\n",
    "        improvement =  prev_cost - cost_val\n",
    "        prev_cost = cost_val\n",
    "        print(\"Step : {}, \\t Cost : {}, \\t Improvement : {}\".format(step, cost_val,  improvement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnist Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda3-52\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-91728e189d13>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3-52\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3-52\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3-52\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3-52\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3-52\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3-52\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "# 맨처음 최초한번만 mnist 다운로딩 이후는 로컬데이터 사용\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784L\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training epoch / batch\n",
    "- epoch : one forward pass and one backward pass of all the training examples\n",
    "- batch : the number of training examples in one forward/backward pass\n",
    "- iterations : number of passes, each pass using [batch size] number of examples \n",
    "\n",
    "- Ex. \n",
    "- if you have 1000 training examples, and your batch size is 500, \n",
    "- then it will take 2 iterations to complete 1 epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\tCost = 3.025496091\n",
      "Epoch: 0002\tCost = 1.125004700\n",
      "Epoch: 0003\tCost = 0.888953560\n",
      "Epoch: 0004\tCost = 0.776706756\n",
      "Epoch: 0005\tCost = 0.706697467\n",
      "Epoch: 0006\tCost = 0.657088381\n",
      "Epoch: 0007\tCost = 0.620253639\n",
      "Epoch: 0008\tCost = 0.590116817\n",
      "Epoch: 0009\tCost = 0.565768591\n",
      "Epoch: 0010\tCost = 0.544986119\n",
      "Epoch: 0011\tCost = 0.527694737\n",
      "Epoch: 0012\tCost = 0.512255847\n",
      "Epoch: 0013\tCost = 0.498603836\n",
      "Epoch: 0014\tCost = 0.486037421\n",
      "Epoch: 0015\tCost = 0.475801935\n",
      "Learning finished\n",
      "----------------------------------------------------------------\n",
      "Accuracy:  0.8883\n",
      "Label:  [5]\n",
      "Prediction:  [5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={\n",
    "                            X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        # print('Epoch:', '%04d' % (epoch + 1),\n",
    "        #       '\\t cost =', '{:.9f}'.format(avg_cost))\n",
    "        print('Epoch:', '%04d' % (epoch + 1), end='\\t')\n",
    "        print('Cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "        \n",
    "    print(\"Learning finished\")\n",
    "    print(\"-\"*2**6)\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(mnist.test.images[r:r + 1].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0001\tCost = 3.024248589\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0002\tCost = 1.127673773\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0003\tCost = 0.889891820\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0004\tCost = 0.777442278\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0005\tCost = 0.706969990\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0006\tCost = 0.657456794\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0007\tCost = 0.620246396\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0008\tCost = 0.590212452\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0009\tCost = 0.564906983\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0010\tCost = 0.545165515\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0011\tCost = 0.527388501\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0012\tCost = 0.511811727\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0013\tCost = 0.498604178\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0014\tCost = 0.486418099\n",
      "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ 100% \n",
      "Epoch: 0015\tCost = 0.475066478\n",
      "\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "sess = tf.Session()\n",
    "# Initialize TensorFlow variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={\n",
    "                        X: batch_xs, Y: batch_ys})\n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "        if i % (total_batch/100) == 0:\n",
    "            print('▒', end='')\n",
    "    \n",
    "    print(' 100% ')\n",
    "    print('Epoch:', '%04d' % (epoch + 1), end='\\t')\n",
    "    print('Cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print()\n",
    "print(\"Learning finished\")\n",
    "\n",
    "# Test the model using test sets\n",
    "# print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "#       X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report result on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8881\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image show and prediction\n",
    "- 랜덤하게 읽어온 이미지를 예측하여 실제 맞는지를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [3]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
